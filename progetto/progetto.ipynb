{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier Project #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset, preprocessing and creating Training and Test set ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('secondary_data.csv', sep=\";\")\n",
    "\n",
    "#preprocessing: convertiamo i valori categorici in numerici\n",
    "df = df.apply(lambda col: pd.factorize(col)[0])\n",
    "\n",
    "def train_test_split(df, test_size):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(22)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "X_train,X_test = train_df.drop(\"class\", axis=1).to_numpy(), test_df.drop(\"class\", axis=1).to_numpy()\n",
    "y_train,y_test = train_df[\"class\"].to_numpy(), test_df[\"class\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gini_impurity() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgini_impurity\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: gini_impurity() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "gini_impurity(X_Train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Node Class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, prediction=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def classify(self, x):\n",
    "        if self.prediction is not None:  # If leaf node\n",
    "            return self.prediction\n",
    "\n",
    "        if x[self.feature_index] < self.threshold:\n",
    "            return self.left_child.classify(x)\n",
    "        else:\n",
    "            return self.right_child.classify(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining split criteria ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(self, y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probas = counts / len(y)\n",
    "    return 1 - np.sum(probas ** 2)\n",
    "\n",
    "def entropy(self, y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probas = counts / len(y)\n",
    "    return -np.sum(probas * np.log2(probas + 1e-9))\n",
    "\n",
    "def misclassification_error(self, y):\n",
    "    majority_class_count = np.max(np.bincount(y))\n",
    "    return 1 - majority_class_count / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreePredictor:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, split_criterion='gini'):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.split_criterion = split_criterion\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y, depth=0)\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        if depth >= self.max_depth or n_samples < self.min_samples_split or len(np.unique(y)) == 1:\n",
    "            leaf_value = self.majority_class(y)\n",
    "            return Node(prediction=leaf_value)\n",
    "        \n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self.majority_class(y)\n",
    "            return Node(prediction=leaf_value)\n",
    "\n",
    "        node = Node(feature_index=best_feature, threshold=best_threshold)\n",
    "\n",
    "        left_idx = X[:, best_feature] < best_threshold\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        # Recursively build left and right child nodes\n",
    "        node.left_child = self.build_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "        node.right_child = self.build_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        best_feature, best_threshold, best_impurity = None, None, float('inf')\n",
    "        current_impurity = self.calculate_impurity(y)\n",
    "\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) > 0 and np.sum(right_mask) > 0:\n",
    "                    left_impurity = self.calculate_impurity(y[left_mask])\n",
    "                    right_impurity = self.calculate_impurity(y[right_mask])\n",
    "                    impurity = (np.sum(left_mask) * left_impurity + np.sum(right_mask) * right_impurity) / len(y)\n",
    "\n",
    "                    if impurity < best_impurity:\n",
    "                        best_impurity = impurity\n",
    "                        best_feature = feature_idx\n",
    "                        best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def calculate_impurity(self, y):\n",
    "        if self.split_criterion == 'gini':\n",
    "            return gini_impurity(self,y)\n",
    "        elif self.split_criterion == 'entropy':\n",
    "            return entropy(self,y)\n",
    "        elif self.split_criterion == 'misclassification':\n",
    "            return misclassification_error(self,y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid splitting criterion: {self.split_criterion}\")\n",
    "\n",
    "    def majority_class(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.root.classify(x) for x in X])\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning and Evaluating ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.63%\n",
      "Testing Accuracy: 99.43%\n"
     ]
    }
   ],
   "source": [
    "# Creating and training the model\n",
    "tree = TreePredictor(max_depth=15, min_samples_split=10, split_criterion='gini')\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model\n",
    "train_accuracy = tree.evaluate(X_train, y_train)\n",
    "test_accuracy = tree.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Prova diverse profonditÃ  massime dell'albero\n",
    "for depth in range(1, 20):\n",
    "    tree = TreePredictor(max_depth=depth, min_samples_split=10, split_criterion='entropy')\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    # Accuratezza sull'insieme di addestramento\n",
    "    train_accuracy = tree.evaluate(X_train, y_train)\n",
    "    training_scores.append(train_accuracy)\n",
    "    \n",
    "    # Accuratezza sull'insieme di test\n",
    "    test_accuracy = tree.evaluate(X_test, y_test)\n",
    "    test_scores.append(test_accuracy)\n",
    "\n",
    "# Grafico della curva di apprendimento\n",
    "plt.plot(range(1, 20), training_scores, label='Training Accuracy')\n",
    "plt.plot(range(1, 20), test_scores, label='Test Accuracy')\n",
    "plt.xlabel('ProfonditÃ  massima dell\\'albero')\n",
    "plt.ylabel('Accuratezza')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Prova diverse profonditÃ  massime dell'albero\n",
    "for depth in range(1, 20):\n",
    "    tree = TreePredictor(max_depth=depth, min_samples_split=10, split_criterion='entropy')\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    # Accuratezza sull'insieme di addestramento\n",
    "    train_accuracy = tree.evaluate(X_train, y_train)\n",
    "    training_scores.append(train_accuracy)\n",
    "    \n",
    "    # Accuratezza sull'insieme di test\n",
    "    test_accuracy = tree.evaluate(X_test, y_test)\n",
    "    test_scores.append(test_accuracy)\n",
    "\n",
    "# Grafico della curva di apprendimento\n",
    "plt.plot(range(1, 20), training_scores, label='Training Accuracy')\n",
    "plt.plot(range(1, 20), test_scores, label='Test Accuracy')\n",
    "plt.xlabel('ProfonditÃ  massima dell\\'albero')\n",
    "plt.ylabel('Accuratezza')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Prova diverse profonditÃ  massime dell'albero\n",
    "for depth in range(1, 20):\n",
    "    tree = TreePredictor(max_depth=depth, min_samples_split=10, split_criterion='missclassification')\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    # Accuratezza sull'insieme di addestramento\n",
    "    train_accuracy = tree.evaluate(X_train, y_train)\n",
    "    training_scores.append(train_accuracy)\n",
    "    \n",
    "    # Accuratezza sull'insieme di test\n",
    "    test_accuracy = tree.evaluate(X_test, y_test)\n",
    "    test_scores.append(test_accuracy)\n",
    "\n",
    "# Grafico della curva di apprendimento\n",
    "plt.plot(range(1, 20), training_scores, label='Training Accuracy')\n",
    "plt.plot(range(1, 20), test_scores, label='Test Accuracy')\n",
    "plt.xlabel('ProfonditÃ  massima dell\\'albero')\n",
    "plt.ylabel('Accuratezza')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'split_criterion': ['gini', 'entropy', 'misclassification']\n",
    "}\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, X_val, y_val, param_grid):\n",
    "    \"\"\"\n",
    "    Funzione che esegue il tuning degli iperparametri testando ogni combinazione di parametri.\n",
    "    \"\"\"\n",
    "    best_params = None\n",
    "    best_accuracy = -1\n",
    "    \n",
    "    # Prova ogni combinazione di iperparametri\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            for split_criterion in param_grid['split_criterion']:\n",
    "                print(f\"Testando: max_depth={max_depth}, min_samples_split={min_samples_split}, split_criterion={split_criterion}\")\n",
    "                \n",
    "                # Crea e addestra il modello\n",
    "                tree = TreePredictor(max_depth=max_depth, min_samples_split=min_samples_split, split_criterion=split_criterion)\n",
    "                tree.fit(X_train, y_train)\n",
    "                \n",
    "                # Valuta il modello sull'insieme di validazione\n",
    "                accuracy = tree.evaluate(X_val, y_val)\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                \n",
    "                # Mantieni traccia della migliore combinazione\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'split_criterion': split_criterion\n",
    "                    }\n",
    "    \n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Eseguiamo il tuning\n",
    "best_params, best_accuracy = hyperparameter_tuning(X_train, y_train, X_val, y_val, param_grid)\n",
    "print(f\"Migliori parametri trovati: {best_params} con accuracy={best_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
